#Starting to write my own word2vec skipgram
 #word2vec is the concept of converting every word in to a vector . This vectors' cells represent lots of
 #charcterstics or maybe  we can say meaning of the word

 #We draw the vector in an embedding space and it is seen that related words
 #lie closer in the embedding space



 #SkipGram Model !

 #How do we make a training set: take a sentence----> "The dog barked"
 #make tuple warnd "DOG"--> (dog,barked) && (dog,the) i.e. (input,output) just froma  sentence
 #Hence unupervised learning !

 




