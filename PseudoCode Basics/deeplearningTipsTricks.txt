What I need to remember:

1. np.exp(x) works for any np.array x and applies the exponential function to every coordinate
2.the sigmoid function and its gradient
3.image2vector is commonly used in deep learning
4.np.reshape is widely used. In the future, I'll see that keeping my matrix/vector dimensions straight will go toward eliminating a lot of bugs.

5. numpy has efficient built-in functions like np.sum(x,axis=1,keepdims=True) etc.

6. broadcasting is extremely useful---> helps especially in Normalizing !

7. that np.dot() performs a matrix-matrix or matrix-vector multiplication. This is different from np.multiply() and the * operator (which is equivalent to .* in Matlab/Octave), which performs an element-wise multiplication.

8. Loss Funcyion::: The loss is used to evaluate the performance of my model. The bigger my loss is, the more different my predictions ( ŷ y^ ) are from the true values ( yy ). In deep learning, I use optimization algorithms like Gradient Descent to train my model and to minimize the cost

9. Vectorization is very important in deep learning. It provides computational efficiency and clarity.

10. I have reviewed the L1 and L2 loss.---> One is based on absand other on square


11. I am familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc...

12. Common steps for pre-processing a new dataset are:

   1.Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)
   2.Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)
   3."Standardize" the data
	 
13. Single layer:
	 
	 - Initialize the parameters of the model
	 - Learn the parameters for the model by minimizing the cost  
	 - Use the learned parameters to make predictions (on the test set)
	 - Analyse the results and conclude
	 
	 Define the model structure (such as number of input features)
	 Initialize the model's parameters
	 Loop:
	 1. Calculate current loss (forward propagation)
	 2.Calculate current gradient (backward propagation)
	 3. Update parameters w and b (gradient descent)
	    w=w-(learning_rate)dw //w is for every feature and so is dw available for everyfeature
			b=b-(learning_rate)db
	 
14.Choice of learning rate
Reminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate  α determines how rapidly we update the parameters. If the learning rate is too large we may "overshoot" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.

15.Week 2 Assignment:
1. Preprocessing the dataset is important. --> Flatten the dataset !
2. I implemented each function separately: initialize(), propagate(), optimize(). Then I built a model().
3. Tuning the learning rate (which is an example of a "hyperparameter") can make a big difference to the algorithm.